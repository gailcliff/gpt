{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f431e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -f input.txt ] || wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfb61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "vocab = sorted(list(set(''.join(text))))\n",
    "\n",
    "print(\"--- vocabulary of dataset ---\")\n",
    "print(repr(''.join(vocab)))\n",
    "print(\"vocab size:\", len(vocab))\n",
    "print()\n",
    "\n",
    "stoi = { s:i for i, s in enumerate(vocab)}\n",
    "itos = { i:s for s, i in stoi.items()}\n",
    "\n",
    "encode = lambda seq: [stoi[ch] for ch in seq]\n",
    "decode = lambda tokens: ''.join([itos[token] for token in tokens])\n",
    "\n",
    "print(\"--- testing encoding and decoding (string: hello world) ---\")\n",
    "print(\"encoding:\", encode('hello, world!'))\n",
    "print(\"decoding:\", decode(encode('hello, world!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1652c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SZ = 256\n",
    "BLOCK_SIZE = 256\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 288\n",
    "NUM_HEADS = 9\n",
    "NUM_DECODER_LAYERS = 6\n",
    "HEAD_SIZE = EMBEDDING_DIM // NUM_HEADS\n",
    "EVAL_ITERS = 200\n",
    "EVAL_INTERVAL = 200\n",
    "DROPOUT = 0.2\n",
    "LR = 3e-4\n",
    "NUM_TRAIN_ITERS = 5000\n",
    "\n",
    "print(f\"--- running on device {DEVICE} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fecb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), device=DEVICE)\n",
    "\n",
    "N = int(len(data) * 0.9)\n",
    "train_data = data[:N]\n",
    "validation_data = data[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa45f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling the data (get a single random batch of the data)\n",
    "def get_batch(split: str):\n",
    "    dataset = train_data if split == 'train' else validation_data\n",
    "\n",
    "    ix = torch.randint(0, len(dataset) - BLOCK_SIZE, (BATCH_SZ,))\n",
    "\n",
    "    contexts = [dataset[pos : pos+BLOCK_SIZE] for pos in ix]\n",
    "    targets = [dataset[pos+1 : pos+BLOCK_SIZE+1] for pos in ix]\n",
    "\n",
    "    contexts, targets = torch.stack(contexts).to(DEVICE), torch.stack(targets).to(DEVICE)\n",
    "\n",
    "    return contexts, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_context, sample_target = get_batch('train')\n",
    "print(\"--- sample batch ---\")\n",
    "print(f\"context:\\n{sample_context}\")\n",
    "print(f\"target:\\n{sample_target}\")\n",
    "\n",
    "print(f\"context[0] decoded: {repr(decode(sample_context[0].tolist()))}, target[0] is {repr(decode(sample_target[0].tolist()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fe06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = nn.Linear(EMBEDDING_DIM, HEAD_SIZE)\n",
    "        self.key = nn.Linear(EMBEDDING_DIM, HEAD_SIZE)\n",
    "        self.value = nn.Linear(EMBEDDING_DIM, HEAD_SIZE)\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        Q = self.query(x) # (B, T, HEAD_SIZE)\n",
    "        K = self.key(x) # (B, T, HEAD_SIZE)\n",
    "\n",
    "        wei = (Q @ K.transpose(-2, -1)) * (HEAD_SIZE**-0.5)\n",
    "\n",
    "        wei = wei.masked_fill(self.mask[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        V = self.value(x)\n",
    "\n",
    "        out = wei @ V # (B, T, HEAD_SIZE)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList(Head() for _ in range(NUM_HEADS))\n",
    "        self.proj = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "        self.dropout = nn.Dropout(DROPOUT) # do dropout regularization at the end of each sub-layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "        out = self.dropout(self.proj(out)) # project back to embedding dimension and apply regularization\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c901b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(EMBEDDING_DIM, 4 * EMBEDDING_DIM),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * EMBEDDING_DIM, EMBEDDING_DIM),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976da768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(EMBEDDING_DIM)\n",
    "        self.mha = MultiHeadAttention()\n",
    "        self.layer_norm2 = nn.LayerNorm(EMBEDDING_DIM)\n",
    "        self.feed_forward = MLP()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.layer_norm1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our bigram model\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "        self.positional_encoding = nn.Embedding(BLOCK_SIZE, EMBEDDING_DIM)\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(NUM_DECODER_LAYERS)])\n",
    "        self.layer_norm = nn.LayerNorm(EMBEDDING_DIM)\n",
    "        self.lm_head = nn.Linear(EMBEDDING_DIM, VOCAB_SIZE)\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        loss = None\n",
    "        B, T = x.shape # (B, T)\n",
    "\n",
    "        input_embedding = self.token_embedding(x)\n",
    "        positional_encoding = self.positional_encoding(torch.arange(T).to(DEVICE))\n",
    "\n",
    "        x = input_embedding + positional_encoding # (B, T, EMBEDDING_DIM)\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        logits = self.lm_head(x) # (B, T, VOCAB_SZ)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C) # reshape to have logits for each single token in the batch all laid out in the frontier\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_new_tokens):\n",
    "        # x: (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # truncate to context window\n",
    "            x_trunc = x[:, -BLOCK_SIZE:]\n",
    "\n",
    "            # forward\n",
    "            logits, loss = self(x_trunc) # (B, T, VOCAB_SIZE)\n",
    "\n",
    "            # last time step of logits for each batch item\n",
    "            logits = logits[:, -1, :] # (B, VOCAB_SIZE)\n",
    "\n",
    "            # softmax for normalized probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # sample\n",
    "            gen = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            x = torch.cat((x, gen), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLM().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, fused=True)\n",
    "\n",
    "print(model)\n",
    "print(\"Number of parameters in model:\", sum(parameter.numel() for parameter in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    losses = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['train', 'validation']:\n",
    "        losses_for_split = torch.zeros(EVAL_ITERS, device=DEVICE)\n",
    "\n",
    "        for i in range(EVAL_ITERS):\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = model(xb, yb)\n",
    "\n",
    "            losses_for_split[i] = loss.item()\n",
    "        \n",
    "        losses[split] = losses_for_split.mean()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5760f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- model generation before training ---\")\n",
    "# print(decode(model.generate((torch.zeros((1, 1), dtype=torch.long, device=DEVICE)), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536eef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "for i in range(NUM_TRAIN_ITERS):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    with autocast():\n",
    "        output = model(input)\n",
    "\n",
    "        logits, loss = model(xb, yb)\n",
    "        estimated_loss = estimate_loss()\n",
    "\n",
    "        if i % EVAL_INTERVAL == 0:\n",
    "            print(f\"--- estimated loss at epoch {i}. train: {estimated_loss['train']}, validation: {estimated_loss['validation']} ---\")\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    # loss.backward()\n",
    "    # optimizer.step()    \n",
    "\n",
    "print(f\"--- loss immediately after training loop: {loss} ---\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- model generation after training ---\")\n",
    "print(decode(model.generate((torch.zeros((1, 1), dtype=torch.long, device=DEVICE)), max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
